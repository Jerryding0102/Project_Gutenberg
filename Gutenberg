import requests, json, os, time, pprint, re
from bs4 import BeautifulSoup
from urllib import parse


listData=[]


url="https://www.gutenberg.org/browse/languages/zh"
headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
rule ={
    
}


## ##########正規1
def getMainlinks():
    response = requests.get(url, headers= headers)
    soup = BeautifulSoup(response.text, "lxml")
    a_elms = soup.select('div.pgdbbylanguage li.pgdbetext a') 
    
    for a in a_elms:
        regex01 = r'[\u4E00-\u9FFF，。：「」；、？！『』]+'
        string01 = a.text
        match01 = re.search(regex01, string01)
        if match01 != None:
            listData.append({
            "title": a.text,
            "link": "https://www.gutenberg.org" + parse.unquote( a.get('href'))
            })
        else:
            continue
        
def getSubLinks():
    for i in range( len(listData) ):
        if "sub" not in listData[i]:
            listData[i]["sub"] = []
        response = requests.get(listData[i]["link"], headers= headers)
        soup = BeautifulSoup(response.text, "lxml")
        a_elms = soup.select('div#download a[type="text/plain; charset=utf-8"]')
        if len(a_elms) > 0 :
            for a in a_elms:
                listData[i]["sub"].append({
                    "sub_title": a.text,
                    "sub_link": "https://www.gutenberg.org" + parse.unquote( a.get("href") )
                })
        else:
            continue
    pprint.pprint(listData)
if __name__ == "__main__":
    time1 = time.time()
    getMainlinks()
    getSubLinks()


## ###########正規2-2
def savejson():
    fp = open("Gutenberg.json" , "w", encoding="utf-8")
    fp.write(json.dumps(listData, ensure_ascii=False) )
    fp.close()
    
    
def writeTxt():
    fp = open("Gutenberg.json" , "r", encoding="utf-8")
    strJson = fp.read()
    fp.close()
    
    folderPath = "Gutenberg3_txt"
    if not os.path.exists(folderPath):
        os.makedirs(folderPath)
    
    listResult = json.loads(strJson)
    for i in range(len(listResult)):
        response = requests.get(listResult[i]["link"], headers= headers )
        soup = BeautifulSoup(response.text, "lxml")
        body = soup.select_one('body')
        strContent = body.text
        regex02 = r'[\u4E00-\u9FFF，。：「」；、？！『』]\r+'
        strContent = re.findall(regex02, strContent) 
        strContent = "".join(strContent)
        
        fileName = f"{listResult[i]['title']}.txt"
        
        fp = open (f"{folderPath}/{fileName}", "w", encoding="utf-8")
        fp.write(listResult[i]["title"])
        fp.write(strContent)
        fp.close()

    
if __name__ == "__main__":
    savejson()
    writeTxt()
    print(f"[total] It takes {time.time() - time1 } seconds.")
